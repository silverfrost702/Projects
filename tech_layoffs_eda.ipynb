{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tech Layoff Trend Analysis (2020–2025) — EDA + Visualization\n",
    "\n",
    "This notebook analyzes tech layoffs across companies, industries, and countries using the Kaggle Layoffs dataset (2020–2025). It delivers:\n",
    "\n",
    "- Monthly timeline chart of layoffs (Plotly)\n",
    "- Industry impact visualization\n",
    "- Correlation analysis with funding stage and funds raised\n",
    "- Interactive Top 10 Most Affected Companies dashboard\n",
    "\n",
    "Dataset auto-discovery looks for filenames containing 'layoff' with extensions .csv, .parquet, .xlsx, or .json within this repository. You can override the dataset path by setting the environment variable `DATASET_PATH` or editing the `dataset_file` selection cell.\n",
    "\n",
    "Note: Optional interactivity uses `ipywidgets`. If it's not available, the notebook will fall back to a static Top 10 chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: install dependencies if needed (uncomment to run)\n",
    "# %pip install pandas numpy plotly ipywidgets openpyxl pyarrow\n",
    "# from google.colab import output; output.enable_custom_widget_manager()  # if running in Colab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, glob, json, warnings\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "from IPython.display import display\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.width = 120\n",
    "pio.templates.default = 'plotly_white'\n",
    "\n",
    "print('pandas', pd.__version__)\n",
    "print('numpy', np.__version__)\n",
    "import plotly; print('plotly', plotly.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locate the dataset\n",
    "The notebook searches common locations for files containing 'layoff' in the name. Set `DATASET_PATH` if auto-discovery fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_DIRS = [Path('.'), Path('..'), Path('/workspace')]\n",
    "NAME_HINTS = ['layoff', 'layoffs', 'tech_layoff', 'layoffs_fyi']\n",
    "EXTS = ['.csv', '.parquet', '.xlsx', '.json']\n",
    "\n",
    "def find_candidate_paths(max_results: int = 200) -> List[Path]:\n",
    "    candidates = []\n",
    "    for root in SEARCH_DIRS:\n",
    "        for hint in NAME_HINTS:\n",
    "            for ext in EXTS:\n",
    "                pattern = f'**/*{hint}*{ext}'\n",
    "                try:\n",
    "                    candidates.extend(root.glob(pattern))\n",
    "                except Exception:\n",
    "                    pass\n",
    "    # Unique, existing, non-hidden, sorted by preferred type and size desc\n",
    "    uniq = []\n",
    "    seen = set()\n",
    "    for p in candidates:\n",
    "        try:\n",
    "            rp = p.resolve()\n",
    "        except Exception:\n",
    "            continue\n",
    "        if not rp.exists():\n",
    "            continue\n",
    "        if rp.suffix.lower() not in EXTS:\n",
    "            continue\n",
    "        s = str(rp)\n",
    "        if any(seg in s for seg in ['/.git/', '/node_modules/']):\n",
    "            continue\n",
    "        if rp not in seen:\n",
    "            uniq.append(rp)\n",
    "            seen.add(rp)\n",
    "    def sort_key(p: Path):\n",
    "        suffix_rank = {'.csv':0,'.parquet':1,'.xlsx':2,'.json':3}.get(p.suffix.lower(),9)\n",
    "        try:\n",
    "            size = p.stat().st_size\n",
    "        except Exception:\n",
    "            size = 0\n",
    "        return (suffix_rank, -size, str(p))\n",
    "    uniq.sort(key=sort_key)\n",
    "    return uniq[:max_results]\n",
    "\n",
    "DATASET_PATH = os.environ.get('DATASET_PATH', '').strip()\n",
    "if DATASET_PATH:\n",
    "    dataset_candidates = [Path(DATASET_PATH).resolve()]\n",
    "else:\n",
    "    dataset_candidates = find_candidate_paths()\n",
    "\n",
    "print(f'Found {len(dataset_candidates)} candidate file(s).')\n",
    "for i, p in enumerate(dataset_candidates[:20], 1):\n",
    "    print(f"{i}. {p}")\n",
    "\n",
    "if not dataset_candidates:\n",
    "    print('No dataset found automatically. Set DATASET_PATH to your file path, e.g.:')\n",
    "    print('  export DATASET_PATH=./data/layoffs.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not dataset_candidates:\n",
    "    raise FileNotFoundError('No layoffs dataset found. Set DATASET_PATH or place the dataset in this repo.')\n",
    "dataset_file = dataset_candidates[0]\n",
    "dataset_file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and clean the dataset\n",
    "This section standardizes column names, parses dates, and harmonizes key fields (company, industry, country, stage, funds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coalesce_series(df: pd.DataFrame, cols, default=None) -> pd.Series:\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            s = df[c]\n",
    "            if s.notna().any():\n",
    "                return s\n",
    "    return pd.Series(default, index=df.index)\n",
    "\n",
    "def standardize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [re.sub(r'[^a-z0-9]+', '_', str(c).strip().lower()).strip('_') for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def load_any(path: Path) -> pd.DataFrame:\n",
    "    suf = path.suffix.lower()\n",
    "    if suf == '.csv':\n",
    "        return pd.read_csv(path)\n",
    "    if suf == '.parquet':\n",
    "        return pd.read_parquet(path)\n",
    "    if suf == '.xlsx':\n",
    "        try:\n",
    "            return pd.read_excel(path, engine='openpyxl')\n",
    "        except Exception:\n",
    "            return pd.read_excel(path)\n",
    "    if suf == '.json':\n",
    "        try:\n",
    "            return pd.read_json(path, lines=True)\n",
    "        except ValueError:\n",
    "            return pd.read_json(path)\n",
    "    raise ValueError(f'Unsupported file type: {suf}')\n",
    "\n",
    "raw_df = load_any(dataset_file)\n",
    "df = standardize_columns(raw_df)\n",
    "\n",
    "# Canonical columns\n",
    "df['company'] = coalesce_series(df, ['company','employer','organization']).astype('string').str.strip()\n",
    "df['industry'] = coalesce_series(df, ['industry','sector']).astype('string').str.strip()\n",
    "df['country'] = coalesce_series(df, ['country','location_country','hq_country','location']).astype('string')\n",
    "\n",
    "def infer_country(val):\n",
    "    if pd.isna(val):\n",
    "        return np.nan\n",
    "    s = str(val)\n",
    "    parts = [p.strip() for p in s.split(',') if p.strip()]\n",
    "    if len(parts) >= 1:\n",
    "        return parts[-1]\n",
    "    return s\n",
    "\n",
    "df['country'] = df['country'].map(infer_country)\n",
    "df['stage'] = coalesce_series(df, ['stage','funding_stage']).astype('string').str.strip()\n",
    "df['funds_raised_millions'] = pd.to_numeric(coalesce_series(df, ['funds_raised_millions','funds_raised_usd_millions','funds_raised_usd_m','raised','funds_millions']), errors='coerce')\n",
    "\n",
    "df['total_laid_off'] = pd.to_numeric(coalesce_series(df, ['total_laid_off','laid_off','num_laid_off','n_laid_off','number_laid_off']), errors='coerce')\n",
    "df['percentage_laid_off'] = pd.to_numeric(coalesce_series(df, ['percentage_laid_off','pct_laid_off','percent_laid_off']), errors='coerce')\n",
    "\n",
    "date_col = None\n",
    "for c in ['date','layoff_date','reported_date','announcement_date','month','event_date']:\n",
    "    if c in df.columns:\n",
    "        date_col = c\n",
    "        break\n",
    "if date_col is None:\n",
    "    raise ValueError('Could not find a date column. Expected one of: date, layoff_date, reported_date, announcement_date, month, event_date')\n",
    "\n",
    "df['date'] = pd.to_datetime(df[date_col], errors='coerce', utc=True).dt.tz_convert(None)\n",
    "if df['date'].isna().all() and pd.api.types.is_numeric_dtype(df[date_col]):\n",
    "    df['date'] = pd.to_datetime(df[date_col], unit='s', errors='coerce')\n",
    "\n",
    "# Time window\n",
    "df = df[(df['date'] >= '2020-01-01') & (df['date'] <= '2025-12-31')]\n",
    "\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month_period'] = df['date'].dt.to_period('M').dt.to_timestamp()\n",
    "df['year_month'] = df['date'].dt.to_period('M').astype(str)\n",
    "\n",
    "df = df[df['total_laid_off'].fillna(0) >= 0]\n",
    "\n",
    "print('Rows after cleaning:', len(df))\n",
    "display(df.head(5))\n",
    "\n",
    "missing = df.isna().mean().sort_values(ascending=False).head(15)\n",
    "display(missing.to_frame('missing_ratio'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate by company, country, year, and month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_company = (\n",
    "    df.groupby(['company'], dropna=False, as_index=False)['total_laid_off']\n",
    "      .sum()\n",
    "      .sort_values('total_laid_off', ascending=False)\n",
    ")\n",
    "\n",
    "by_company_year = (\n",
    "    df.groupby(['company','year'], as_index=False)['total_laid_off'].sum()\n",
    ")\n",
    "\n",
    "by_country_year = (\n",
    "    df.groupby(['country','year'], as_index=False)['total_laid_off']\n",
    "      .sum()\n",
    "      .sort_values(['year','total_laid_off'], ascending=[True, False])\n",
    ")\n",
    "\n",
    "by_industry_year = (\n",
    "    df.groupby(['industry','year'], as_index=False)['total_laid_off']\n",
    "      .sum()\n",
    "      .sort_values(['year','total_laid_off'], ascending=[True, False])\n",
    ")\n",
    "\n",
    "monthly = (\n",
    "    df.groupby('month_period', as_index=False)['total_laid_off']\n",
    "      .sum()\n",
    "      .sort_values('month_period')\n",
    ")\n",
    "\n",
    "display(by_company.head(10))\n",
    "display(monthly.tail(6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monthly layoff trend (Plotly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_timeline = px.line(\n",
    "    monthly, x='month_period', y='total_laid_off',\n",
    "    title='Monthly Tech Layoffs (2020–2025)', markers=True\n",
    ")\n",
    "fig_timeline.update_layout(\n",
    "    yaxis_title='Total Laid Off', xaxis_title='Month', hovermode='x unified'\n",
    ")\n",
    "fig_timeline.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which industries were hit hardest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry_totals = (\n",
    "    df.groupby('industry', as_index=False)['total_laid_off']\n",
    "      .sum()\n",
    "      .sort_values('total_laid_off', ascending=False)\n",
    ")\n",
    "top_industries = industry_totals.head(20)\n",
    "fig_industry = px.bar(\n",
    "    top_industries, x='industry', y='total_laid_off',\n",
    "    title='Industries Most Affected by Layoffs', text='total_laid_off'\n",
    ")\n",
    "fig_industry.update_traces(texttemplate='%{text:.0f}', textposition='outside')\n",
    "fig_industry.update_layout(xaxis={'categoryorder':'total descending'}, yaxis_title='Total Laid Off')\n",
    "fig_industry.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation with funding stage or size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_cols = [c for c in ['total_laid_off','percentage_laid_off','funds_raised_millions'] if c in df.columns]\n",
    "if corr_cols:\n",
    "    corr = df[corr_cols].corr(numeric_only=True)\n",
    "    fig_corr = px.imshow(corr, text_auto=True, aspect='auto', title='Correlation Matrix')\n",
    "    fig_corr.show()\n",
    "else:\n",
    "    print('No numeric columns available for correlation matrix.')\n",
    "\n",
    "if 'funds_raised_millions' in df.columns:\n",
    "    scatter_df = df.dropna(subset=['funds_raised_millions','total_laid_off']).copy()\n",
    "    if len(scatter_df) > 0:\n",
    "        color_col = 'stage' if 'stage' in df.columns else None\n",
    "        fig_scatter = px.scatter(\n",
    "            scatter_df, x='funds_raised_millions', y='total_laid_off', color=color_col,\n",
    "            title='Layoffs vs. Funds Raised', labels={'funds_raised_millions':'Funds Raised (USD millions)'}\n",
    "        )\n",
    "        fig_scatter.update_xaxes(type='log')\n",
    "        fig_scatter.show()\n",
    "    else:\n",
    "        print('Insufficient data for layoffs vs. funds raised scatter.')\n",
    "else:\n",
    "    print('Column funds_raised_millions not found; skipping scatter.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 10 Most Affected Companies — interactive dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_topn_bar(sub_df: pd.DataFrame, topn: int = 10):\n",
    "    top = (sub_df.groupby('company', as_index=False)['total_laid_off']\n",
    "             .sum().sort_values('total_laid_off', ascending=False).head(topn))\n",
    "    if len(top) == 0:\n",
    "        print('No data for current filters.')\n",
    "        return\n",
    "    fig = px.bar(top, x='company', y='total_laid_off', title=f'Top {topn} Companies by Layoffs', text='total_laid_off')\n",
    "    fig.update_traces(texttemplate='%{text:.0f}', textposition='outside')\n",
    "    fig.update_layout(xaxis={'categoryorder':'total descending'}, yaxis_title='Total Laid Off')\n",
    "    fig.show()\n",
    "\n",
    "# Try ipywidgets; if unavailable, fall back to static chart\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display, clear_output\n",
    "    years = ['All'] + sorted(pd.Series(df['year'].dropna().unique()).astype(int).tolist())\n",
    "    industries = ['All'] + sorted([str(x) for x in df['industry'].dropna().unique()])\n",
    "    countries = ['All'] + sorted([str(x) for x in df['country'].dropna().unique()])\n",
    "    w_year = widgets.Dropdown(options=years, value='All', description='Year:')\n",
    "    w_industry = widgets.Dropdown(options=industries, value='All', description='Industry:')\n",
    "    w_country = widgets.Dropdown(options=countries, value='All', description='Country:')\n",
    "    w_topn = widgets.IntSlider(value=10, min=5, max=30, step=1, description='Top N')\n",
    "    out = widgets.Output()\n",
    "\n",
    "    def update_dashboard(*args):\n",
    "        with out:\n",
    "            clear_output(wait=True)\n",
    "            mask = pd.Series(True, index=df.index)\n",
    "            if w_year.value != 'All':\n",
    "                mask &= df['year'] == int(w_year.value)\n",
    "            if w_industry.value != 'All':\n",
    "                mask &= df['industry'] == w_industry.value\n",
    "            if w_country.value != 'All':\n",
    "                mask &= df['country'] == w_country.value\n",
    "            sub = df[mask] if mask.any() else df.iloc[0:0]\n",
    "            render_topn_bar(sub, topn=int(w_topn.value))\n",
    "\n",
    "    for w in [w_year, w_industry, w_country, w_topn]:\n",
    "        w.observe(update_dashboard, names='value')\n",
    "\n",
    "    display(widgets.HBox([w_year, w_industry, w_country, w_topn]))\n",
    "    display(out)\n",
    "    update_dashboard()\n",
    "except Exception as e:\n",
    "    print('ipywidgets not available or failed to render widgets. Showing static Top 10 chart.')\n",
    "    render_topn_bar(df, topn=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- If auto-discovery did not find your dataset, set `DATASET_PATH` to your file path and re-run the Load section.\n",
    "- Column names may vary across Kaggle versions. The cleaning code attempts to coalesce common variants and gracefully skip missing columns.\n",
    "- For the funding correlation, the strongest signals often appear with larger samples; filter by year or industry to explore segment-specific trends."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}